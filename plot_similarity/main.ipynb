{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_s = [\"This concept of distance is not restricted to two dimensions.\", \"This concept of distance is not restricted to two dimensions.\"]\n",
    "corpus = [\n",
    "        'This concept of distance is not restricted to two dimension.',\n",
    "        'This is the first document.',\n",
    "        'This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?',\n",
    "        'It is not difficult to imagine the figure above translated into three dimensions.',\n",
    "        'We can persuade ourselves that the measure of distance extends to an arbitrary number of dimensions;',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    text_stem = [stemmer.stem(token) for token in text.split(' ')]\n",
    "    text_stem_join = ' '.join(text_stem)\n",
    "    return text_stem_join\n",
    "\n",
    "corpus_stem = list(map(stem, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - Show the tfidf.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abov', 'arbitrari', 'concept', 'difficult', 'dimension', 'dimensions', 'distanc', 'document', 'extend', 'figur', 'imagin', 'measur', 'number', 'ourselv', 'persuad', 'restrict', 'second', 'thi', 'translat']\n",
      "[[ 0.          0.          0.50865318  0.          0.50865318  0.\n",
      "   0.38897149  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.50865318  0.          0.26928979  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.7640961   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.64510243  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.29006559  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.92514281  0.2448933   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.7640961   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.64510243  0.        ]\n",
      " [ 0.42315258  0.          0.          0.42315258  0.          0.32358844\n",
      "   0.          0.          0.          0.42315258  0.42315258  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.42315258]\n",
      " [ 0.          0.37346834  0.          0.          0.          0.28559447\n",
      "   0.28559447  0.          0.37346834  0.          0.          0.37346834\n",
      "   0.37346834  0.37346834  0.37346834  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=False, stop_words='english')\n",
    "\n",
    "X = tfidf.fit_transform(corpus_stem)\n",
    "print(tfidf.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance 0 -> 1:  0.826280498813\n",
      "cosine distance 0 -> 2:  0.934052732742\n",
      "cosine similarity 0 -> 1:  [[ 0.1737195]]\n",
      "cosine similarity 0 -> 2:  [[ 0.06594727]]\n",
      "cosine similarity 0 -> 3:  [[ 0.]]\n",
      "cosine similarity 0 -> 4:  [[ 0.1737195]]\n",
      "cosine similarity 0 -> 5:  [[ 0.]]\n",
      "cosine similarity 0 -> 6:  [[ 0.11108811]]\n"
     ]
    }
   ],
   "source": [
    "print('cosine distance 0 -> 1: ', cosine(X[0].toarray(), X[1].toarray()))\n",
    "print('cosine distance 0 -> 2: ', cosine(X[0].toarray(), X[2].toarray()))\n",
    "print('cosine similarity 0 -> 1: ', cosine_similarity(X[0].toarray(), X[1].toarray()))\n",
    "print('cosine similarity 0 -> 2: ', cosine_similarity(X[0].toarray(), X[2].toarray()))\n",
    "print('cosine similarity 0 -> 3: ', cosine_similarity(X[0].toarray(), X[3].toarray()))\n",
    "print('cosine similarity 0 -> 4: ', cosine_similarity(X[0].toarray(), X[4].toarray()))\n",
    "print('cosine similarity 0 -> 5: ', cosine_similarity(X[0].toarray(), X[5].toarray()))\n",
    "print('cosine similarity 0 -> 6: ', cosine_similarity(X[0].toarray(), X[6].toarray()))\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Create a sparse matrix of similarities between every article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Plot the sparse matrix of similarities using a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Create a wordcloud of the feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/'\n",
    "plots_filename = 'story_plots_AA.txt'\n",
    "titles_filename = 'story_titles_AA.txt'\n",
    "separator = '<EOS>'\n",
    "\n",
    "with open(dataset_dir + plots_filename, 'r') as file:\n",
    "    corpus = file.readlines()\n",
    "    corpus = corpus[:-1]\n",
    "    corpus = ''.join(corpus)\n",
    "    corpus = corpus.split(separator)\n",
    "\n",
    "with open(dataset_dir + titles_filename, 'r') as file:\n",
    "    titles = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.032130</td>\n",
       "      <td>0.032563</td>\n",
       "      <td>0.017297</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.035305</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>0.019549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>0.019891</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>0.024427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029778</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>0.020720</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>0.036014</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.035046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032130</td>\n",
       "      <td>0.029778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066226</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>0.031403</td>\n",
       "      <td>0.065506</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.039239</td>\n",
       "      <td>0.030849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033170</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>0.025155</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.035315</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.040421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.032563</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>0.066226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.113380</td>\n",
       "      <td>0.072492</td>\n",
       "      <td>0.058612</td>\n",
       "      <td>0.039950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052293</td>\n",
       "      <td>0.028155</td>\n",
       "      <td>0.053332</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>0.055759</td>\n",
       "      <td>0.049453</td>\n",
       "      <td>0.037970</td>\n",
       "      <td>0.042873</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.054051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017297</td>\n",
       "      <td>0.020720</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.047575</td>\n",
       "      <td>0.022910</td>\n",
       "      <td>0.050548</td>\n",
       "      <td>0.036349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034595</td>\n",
       "      <td>0.013046</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>0.031151</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>0.026737</td>\n",
       "      <td>0.029003</td>\n",
       "      <td>0.012626</td>\n",
       "      <td>0.030277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.000000  0.014620  0.032130  0.032563  0.017297  0.015070  0.035305   \n",
       "1  0.014620  1.000000  0.029778  0.030944  0.020720  0.022432  0.036014   \n",
       "2  0.032130  0.029778  1.000000  0.066226  0.036943  0.031403  0.065506   \n",
       "3  0.032563  0.030944  0.066226  1.000000  0.044317  0.039795  0.113380   \n",
       "4  0.017297  0.020720  0.036943  0.044317  1.000000  0.015940  0.047575   \n",
       "\n",
       "         7         8         9     ...           34        35        36  \\\n",
       "0  0.016205  0.019628  0.019549    ...     0.019277  0.011591  0.019662   \n",
       "1  0.018200  0.019568  0.023523    ...     0.034594  0.015246  0.020325   \n",
       "2  0.028701  0.039239  0.030849    ...     0.033170  0.018685  0.032358   \n",
       "3  0.072492  0.058612  0.039950    ...     0.052293  0.028155  0.053332   \n",
       "4  0.022910  0.050548  0.036349    ...     0.034595  0.013046  0.037634   \n",
       "\n",
       "         37        38        39        40        41        42        43  \n",
       "0  0.015654  0.024832  0.018403  0.019891  0.019999  0.011042  0.024427  \n",
       "1  0.016751  0.023272  0.016368  0.018685  0.018161  0.020093  0.035046  \n",
       "2  0.025155  0.038645  0.024923  0.028372  0.035315  0.024307  0.040421  \n",
       "3  0.038874  0.055759  0.049453  0.037970  0.042873  0.058735  0.054051  \n",
       "4  0.031151  0.025615  0.021572  0.026737  0.029003  0.012626  0.030277  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_stem = list(map(stem, corpus))\n",
    "tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=False, stop_words='english')\n",
    "\n",
    "X = tfidf.fit_transform(corpus_stem)\n",
    "# print(tfidf.get_feature_names())\n",
    "# print(X.toarray())\n",
    "\n",
    "N = len(X.toarray())\n",
    "a = np.zeros(shape=(N,N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        a[i][j] = cosine_similarity(X[i].toarray(), X[j].toarray())[0][0]\n",
    "        \n",
    "# df = pd.DataFrame(data=a, columns=titles, index=titles)\n",
    "df = pd.DataFrame(data=a)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install nltk\n",
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df == df.max()] = 0 # 1's become 0's\n",
    "most_similar = np.array(df.apply(lambda x: df.columns[x.argmax()], axis = 1)) # column name of the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar article to Citizen Kane\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Animal Farm\n",
      " is: Batman (1966 film)\n",
      "\n",
      "Most similar article to A Clockwork Orange (novel)\n",
      " is: The Big O\n",
      "\n",
      "Most similar article to The Plague\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Actaeon\n",
      " is: A Funny Thing Happened on the Way to the Forum\n",
      "\n",
      "Most similar article to A Fire Upon the Deep\n",
      " is: Babylon 5\n",
      "\n",
      "Most similar article to All Quiet on the Western Front\n",
      " is: The Plague\n",
      "\n",
      "Most similar article to Anyone Can Whistle\n",
      " is: The Plague\n",
      "\n",
      "Most similar article to A Funny Thing Happened on the Way to the Forum\n",
      " is: The Plague\n",
      "\n",
      "Most similar article to Army of Darkness\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to The Birth of a Nation\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Blade Runner\n",
      " is: Blade Runner 2: The Edge of Human\n",
      "\n",
      "Most similar article to Blazing Saddles\n",
      " is: A Clockwork Orange (novel)\n",
      "\n",
      "Most similar article to Blue Velvet (film)\n",
      " is: Buffy the Vampire Slayer (film)\n",
      "\n",
      "Most similar article to Blade Runner 2: The Edge of Human\n",
      " is: Blade Runner\n",
      "\n",
      "Most similar article to Barry Lyndon\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Buffy the Vampire Slayer (film)\n",
      " is: Blue Velvet (film)\n",
      "\n",
      "Most similar article to The Big O\n",
      " is: A Clockwork Orange (novel)\n",
      "\n",
      "Most similar article to Braveheart\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Bank of China Tower (Hong Kong)\n",
      " is: Chariots of Fire\n",
      "\n",
      "Most similar article to Batman (1989 film)\n",
      " is: Batman: Year One\n",
      "\n",
      "Most similar article to Batman (1966 film)\n",
      " is: Batman Forever\n",
      "\n",
      "Most similar article to Batman Returns\n",
      " is: Batman (1966 film)\n",
      "\n",
      "Most similar article to Batman &amp; Robin (film)\n",
      " is: Batman Forever\n",
      "\n",
      "Most similar article to Batman Forever\n",
      " is: Batman (1989 film)\n",
      "\n",
      "Most similar article to Batman: Year One\n",
      " is: Batman (1989 film)\n",
      "\n",
      "Most similar article to Bubblegum Crisis\n",
      " is: Chrono Trigger\n",
      "\n",
      "Most similar article to Babylon 5\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Bildungsroman\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Bride of the Monster\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Crouching Tiger, Hidden Dragon\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Chariots of Fire\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Crash (J. G. Ballard novel)\n",
      " is: Braveheart\n",
      "\n",
      "Most similar article to Castle of the Winds\n",
      " is: Chrono Cross\n",
      "\n",
      "Most similar article to Chrono Trigger\n",
      " is: Chrono Cross\n",
      "\n",
      "Most similar article to Chapterhouse: Dune\n",
      " is: Vladimir Harkonnen\n",
      "\n",
      "Most similar article to Carmilla\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Cowboy Bebop\n",
      " is: Anyone Can Whistle\n",
      "\n",
      "Most similar article to Carousel (musical)\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to The Cider House Rules\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Chrono Cross\n",
      " is: Chrono Trigger\n",
      "\n",
      "Most similar article to Vladimir Harkonnen\n",
      " is: All Quiet on the Western Front\n",
      "\n",
      "Most similar article to Destry Rides Again\n",
      " is: Blazing Saddles\n",
      "\n",
      "Most similar article to Dracula\n",
      " is: The Plague\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(most_similar)):\n",
    "    print('Most similar article to', titles[i], 'is:', titles[most_similar[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar articles: Chapterhouse: Dune\n",
      " and Vladimir Harkonnen\n",
      "\n",
      "Plot of Chapterhouse: Dune\n",
      "\n",
      "\n",
      "\n",
      "The Bene Gesserit still find themselves questioning the Golden Path of humanity set by the God Emperor Leto II.\n",
      "Now they must survive the Honored Matres, whose reckless conquest of the Old Empire threatens Bene Gesserit survival.\n",
      "The Sisters must reassess their timeless methods: does ultimate survival go beyond calculated manipulation.\n",
      "Is there greater purpose to life than consolidating power.\n",
      "The situation is desperate for the Bene Gesserit as they find themselves the targets of the Honored Matres, whose conquest of the Old Empire is almost complete.\n",
      "The Matres are seeking to assimilate the technology and developed methods of the Bene Gesserit and exterminate the Sisterhood itself.\n",
      "Now in command of the Bene Gesserit, Mother Superior Darwi Odrade continues to develop her drastic, secret plan to overcome the Honored Matres.\n",
      "The Bene Gesserit are also terraforming the planet Chapterhouse to accommodate the all-important sandworms, whose native planet Dune had been destroyed by the Matres.\n",
      "Sheeana, in charge of the project, expects sandworms to appear soon.\n",
      "The Honored Matres have also destroyed the entire Bene Tleilax civilization, with Tleilaxu Master Scytale the only one of his kind left alive.\n",
      "In Bene Gesserit captivity, Scytale possesses the Tleilaxu secret of ghola production, which he has reluctantly traded for the Sisterhood's protection.\n",
      "The first ghola produced is that of their recently deceased military genius, Miles Teg.\n",
      "The Bene Gesserit have two other prisoners on Chapterhouse: the latest Duncan Idaho ghola, and former Honored Matre Murbella, whom they have accepted as a novice despite their suspicion that she intends to escape back to the Honored Matres.\n",
      "Lampadas, a center for Bene Gesserit education, has been destroyed by the Honored Matres.\n",
      "The planet's Chancellor, Reverend Mother Lucilla, manages to escape carrying the shared-minds of millions of Reverend Mothers.\n",
      "Lucilla is forced to land on Gammu where she seeks refuge with an underground group of Jews.\n",
      "The Rabbi gives Lucilla sanctuary, but to save his organization he must deliver her to the Matres.\n",
      "Before doing so, he reveals Rebecca, a \"wild\" Reverend Mother who has gained her Other Memory without Bene Gesserit training.\n",
      "Lucilla shares minds with Rebecca, who promises to take the memories of Lampadas safely back to the Sisterhood.\n",
      "Lucilla is then \"betrayed\", and taken before the Great Honored Matre Dama, who tries to persuade her to join the Honored Matres, preserving her life in exchange for Bene Gesserit secrets.\n",
      "Lucilla refuses, and Dama ultimately kills her.\n",
      "Back on Chapterhouse, Odrade confronts Duncan and forces him to admit that he is a Mentat, proving that he retains the memories of his many ghola lives.\n",
      "He does not reveal his mysterious visions of two people.\n",
      "Meanwhile, Murbella collapses under the pressure of Bene Gesserit training, giving in to \"word weapons\" that the Bene Gesserit had planted to undermine her earlier Honored Matre identity.\n",
      "Murbella realizes that she wants to be Bene Gesserit.\n",
      "Odrade believes that the Bene Gesserit made a mistake in fearing emotion, and that in order to evolve, the Bene Gesserit must learn to accept emotions.\n",
      "Odrade permits Duncan to watch Murbella undergo the spice agony, making him the first man ever to do so.\n",
      "Murbella survives the ordeal and becomes a Reverend Mother.\n",
      "Odrade then confronts Sheeana, discovering that Duncan and Sheeana have been allied together for some time.\n",
      "Sheeana does not reveal that they have been considering the option of reawakening Teg's memory through Imprinting, nor does Odrade discover that Sheeana has the keys to Duncan's no-ship prison.\n",
      "Odrade continues molding Scytale, with Sheeana showing him a baby sandworm, the Bene Gesserit's own long term supply of spice, and destroying Scytale's main bargaining card.\n",
      "Finally, Teg is awakened by Sheeana using imprinting techniques.\n",
      "Odrade appoints him again as Bashar of the military forces of the Sisterhood for the assault on the Honored Matres.\n",
      "Odrade next calls a meeting of all the Bene Gesserit, announcing her plan to attack the Honored Matres.\n",
      "She tells them that this attack will be led by Teg.\n",
      "She also announces candidates to succeed her as Mother Superior; she will share her memories with Murbella and Sheeana before she leaves.\n",
      "Odrade then goes to meet the Great Honored Matre.\n",
      "Under cover of Odrade's diplomacy, the Bene Gesserit forces under Teg attack Gammu with tremendous force.\n",
      "Teg uses his secret ability to see no-ships to secure control of the system.\n",
      "Survivors of the attack flee to Junction, and Teg follows them there and carries all with him.\n",
      "Victory for the Bene Gesserit seems inevitable.\n",
      "In the midst of this battle, the Jews (including Rebecca with her precious memories) take refuge with the Bene Gesserit fleet.\n",
      "Logno — chief advisor to Dama — assassinates Dama with poison and assumes control of the Honored Matres.\n",
      "Her first act surprises Odrade greatly.\n",
      "Too late, Odrade and Teg realize they have fallen into a trap, and the Honored Matres use a mysterious weapon to turn defeat into victory, as well as capturing Odrade.\n",
      "Murbella saves as much of the Bene Gesserit force as she can and they begin to withdraw to Chapterhouse.\n",
      "Odrade, however, had planned for the possible failure of the Bene Gesserit attack and left Murbella instructions for a last desperate gamble.\n",
      "Murbella pilots a small craft down to the surface, announcing herself as an Honored Matre who, in the confusion, has managed to escape the Bene Gesserit with all their secrets.\n",
      "She arrives on the planet and is taken to the Great Honored Matre.\n",
      "Unable to control her anger, Logno attacks but is killed by Murbella.\n",
      "Awed by her physical prowess, the remaining Honored Matres are forced to accept her as their new leader.\n",
      "Odrade is also killed in the melee and Murbella shares with Odrade to absorb her newest memories, as they had already shared prior to the battle.\n",
      "Murbella's ascension to leadership is not accepted as victory by all the Bene Gesserit.\n",
      "Some flee Chapterhouse, notably Sheeana, who has a vision of her own, and arranges to have some of the new worms that have emerged in the Chapterhouse desert brought aboard the no-ship.\n",
      "Sheeana is joined by Duncan.\n",
      "The two escape in the giant no-ship, with Scytale, Teg and the Jews.\n",
      "Murbella recognizes their plan at the last minute, but is powerless to stop them.\n",
      "Watching this escape with interest are Daniel and Marty, the observers of whom Duncan had been having visions.\n",
      "The story ends on a cliffhanger with several questions left unanswered regarding the merging of the Honored Matres and Bene Gesserit, the fates of those on the escaped no-ship (including the role of Scytale, the development of Idaho and Teg, and the role of the Jews), the identity of the god-like characters in the book's final chapter and the ultimate mystery of what chased the Honored Matres back into the Old Empire.\n",
      "\n",
      "\n",
      "\n",
      "Plot of Vladimir Harkonnen\n",
      "\n",
      "\n",
      "\n",
      "As Dune begins, a longstanding feud exists between the Harkonnens of Giedi Prime and the Atreides of Caladan.\n",
      "The Baron's intent to exterminate the Atreides line seems close to fruition as Duke Leto Atreides is lured to the desert planet Arrakis on the pretense of taking over the valuable melange operation there.\n",
      "The Baron has an agent in the Atreides household: Leto's own physician, the trusted Suk doctor Wellington Yueh.\n",
      "Though Suk Imperial Conditioning supposedly makes the subject incapable of inflicting harm, the Baron's twisted Mentat Piter De Vries notes:  It's assumed that ultimate conditioning cannot be removed without killing the subject.\n",
      "However, as someone once observed, given the right lever you can move a planet.\n",
      "We found the lever that moved the doctor.\n",
      "The Baron has taken Yueh's wife Wanna prisoner, threatening her with interminable torture unless Yueh complies with his demands.\n",
      "Harkonnen also distracts Leto's Mentat Thufir Hawat from discovering Yueh by guiding Hawat toward another suspect: Leto's Bene Gesserit concubine Lady Jessica.\n",
      "The Atreides are soon attacked by Harkonnen forces (secretly supplemented by the seemingly unstoppable Imperial Sardaukar) as Yueh disables the protective shields around the Atreides palace on Arrakis.\n",
      "As instructed, Yueh takes Leto prisoner; however, desiring to slay the Baron, Yueh provides the captive Leto with a fake tooth filled with poisonous gas as a means of simultaneous assassination and suicide.\n",
      "De Vries kills Yueh but he also dies with Leto in the assassination attempt; however Harkonnen survives.\n",
      "The Baron then manipulates Hawat into his service, by convincing Hawat that Lady Jessica was the traitor and using Hawat's desire for revenge on Lady Jessica and the Emperor as motivation to assist House Harkonnen.\n",
      "Leto and Jessica's son Paul Atreides flee into the desert with Jessica, and both are presumed dead.\n",
      "Paul's prescience helps him determine the identity of Jessica's father, the \"maternal grandfather who cannot be named\" — the Baron himself.\n",
      "Over the next two years, Harkonnen learns that his nephews Glossu Rabban and Feyd-Rautha are conspiring against him to usurp his throne; he lets them continue to do so, reasoning that they have to somehow learn to organize a conspiracy.\n",
      "As punishment for a failed assassination attempt against him, Harkonnen forces Feyd to single-handedly slaughter all the female slaves who serve as Feyd's lovers.\n",
      "He explains that Feyd has to learn the price of failure.\n",
      "The Baron's plan to assure Feyd's power is to install him as ruler of Arrakis after a period of tyrannical misrule by Rabban, making Feyd appear to be the savior of the people.\n",
      "However, a crisis on Arrakis begins when the mysterious Muad'Dib emerges as a leader of the native Fremen tribes against the rule of the Harkonnens.\n",
      "Eventually, a series of Fremen victories against Beast Rabban threaten to disrupt the trade of the spice.\n",
      "The Padishah Emperor Shaddam IV decides to intervene himself and arrives on Arrakis along with legions of Sardaukar forces.\n",
      "Shaddam and the Baron are shocked to learn that Muad'Dib is, of course, a very-much-alive Paul Atreides.\n",
      "The Imperial forces fall prey to a surprise attack by the Fremen.\n",
      "Part of the Fremen/Atreides strategy is to wait until a sandstorm shorts out the force field shields of the Harkonnen/Imperial transport ships, disable them with projectile weapons, and then attack with a vast assault force, using sandworms under cover of the severe weather to break the enemy lines.\n",
      "The Sardaukar and Harkonnen forces are trapped on the planet, astonished at the sandworm mounts and vast numbers of their attackers.\n",
      "Their past ruthlessness gives them little hope of quarter from the enraged Fremen.\n",
      "Rabban dies in the initial part of the battle; the Harkonnen army is massacred to the last man and almost all the Imperial Sardaukar are killed.\n",
      "Baron Harkonnen himself is poisoned with a gom jabbar by Paul's young sister Alia Atreides, his own granddaughter, and dies at the age of 83, with the latter also revealing her direct lineage to him just beforehand.\n",
      "Paul then kills Feyd in ritual combat.\n",
      "House Harkonnen's virtual extermination removes it as a galactic power, but Paul's ascension to the Imperial throne in Shaddam's place guarantees that Vladimir's descendants will long reign as the Imperial House Atreides.\n",
      "Alia had been born with her ancestral memories in the womb, a circumstance the Bene Gesserit refer to as Abomination, because in their experience it is inevitable that the individual will become possessed by the personality of one of their ancestors.\n",
      "In Children of Dune, Alia falls victim to this prediction when she shares control of her body with the ego-memory of the Baron Harkonnen, and eventually falls under his power.\n",
      "Alia eventually commits suicide, realizing that Harkonnen's consciousness has surpassed her abilities to contain him.\n",
      "In the Prelude to Dune prequel series by Brian Herbert and Anderson, it is established that Baron Vladimir Harkonnen is the son and heir of Dmitri Harkonnen and his wife Victoria.\n",
      "Harkonnen's father had been the head of House Harkonnen and ruled the planet Giedi Prime.\n",
      "Trained since youth as a possible successor, Vladimir had been eventually chosen over his half-brother Abulurd (namesake of the original).\n",
      "Unhappy with his brother's doings, Abulurd eventually marries Emmi Rabban and renounces the family name and his rights to the title.\n",
      "Under the name Abulurd Rabban, he reigns as governor of the secondary Harkonnen planet Lankiveil.\n",
      "Abulurd and his wife have two sons: Glossu Rabban (later nicknamed \"Beast Rabban\" after he murders his own father) and Feyd-Rautha; Vladimir later adopts the boys back into House Harkonnen, and Feyd becomes his designated heir.\n",
      "The Baron's most prominent political rival is Duke Leto Atreides; the Harkonnens and the Atreides have been bitter enemies for millennia, since the Battle of Corrin that ended the Butlerian Jihad.\n",
      "When Emperor Shaddam IV orchestrates a plot to destroy the \"Red Duke\" Leto, the Baron eagerly lends his aid.\n",
      "The young Baron Vladimir is described as an exceedingly handsome man, possessing red hair and a near-perfect physique.\n",
      "The Bene Gesserit Reverend Mother Gaius Helen Mohiam is instructed by the Sisterhood to collect the genetic material of Baron Vladimir Harkonnen (through conception) for their breeding program.\n",
      "As the Baron's homosexuality is something of an open secret, Mohiam blackmails him into having sexual relations with her and conceives his child.\n",
      "When that daughter proves genetically undesirable, Mohiam kills her and returns to Harkonnen for a second try; at this point he drugs and viciously rapes her.\n",
      "She exacts her retribution by infecting him with a rare, incurable disease that later causes his obesity.\n",
      "Mohiam's second child with the Baron is Jessica.\n",
      "In , the deteriorating Baron at first walks with the assistance of a cane, then relies on belt-mounted suspensors to retain mobility.\n",
      "He consults numerous doctors in the expanse of time between the and Dune: House Harkonnen, up to and including his future instrument dr Yueh, all of whom are ultimately no help.\n",
      "To conceal this debilitation, he pretends that his obesity is due to intentional overindulgence, lest the Landsraad remove him from power.\n",
      "When he determines that Mohiam inflicted him with the disease, he attempts to coerce her into revealing the cure, but soon discovers that there is none.\n",
      "The Baron, Duke Leto, and Jessica herself are unaware that Jessica is secretly the Baron's daughter or that he has even fathered one; in the year 10,176, the Baron's grandson Paul is born to Leto and Jessica.\n",
      "In Hunters of Dune (2006), the continuation of the original series by Brian Herbert and Kevin Anderson, the Baron is resurrected as a ghola (5,029 years after the death of Alia) by the Lost Tleilaxu Uxtal, acting on orders from the Face Dancer Khrone.\n",
      "Khrone intends to use the Baron ghola to manipulate a ghola of Paul Atreides, named Paolo.\n",
      "Khrone tries various torture techniques for three years to awaken the 12-year-old Baron's genetic memories; these methods fail due to the Baron's sadomasochistic nature.\n",
      "Khrone is successful when he imprisons the Baron in a sensory deprivation tank for a prolonged period; the Baron's memories of his former life return.\n",
      "Ironically, the reincarnated Baron is soon haunted by the voice of Alia in his mind; the source of this inner Alia is never explained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Most similar articles:', titles[most_similar.argmax()], 'and', titles[most_similar.max()])\n",
    "print('Plot of', titles[most_similar.argmax()])\n",
    "print()\n",
    "print(corpus[most_similar.argmax()])\n",
    "print()\n",
    "print()\n",
    "print('Plot of', titles[most_similar.max()])\n",
    "print()\n",
    "print(corpus[most_similar.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/bmartins/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6155061d27a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mngram_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 238\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bmartins/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/bmartins/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 1), min_df=1)\n",
    "X = ngram_vectorizer.fit_transform(corpus[most_similar.max()])\n",
    "vocab = list(ngram_vectorizer.get_feature_names())\n",
    "counts = X.sum(axis=0).A1\n",
    "freq_distribution = Counter(dict(zip(vocab, counts)))\n",
    "print(freq_distribution.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib\n",
    "\n",
    "sns.heatmap(df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
