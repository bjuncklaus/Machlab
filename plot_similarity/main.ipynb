{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "---\n",
    "Downloaded from https://dumps.wikimedia.org/enwiki/ and used this method https://github.com/markriedl/WikiPlots to extract the info and help recreate the corpus.\n",
    "\n",
    "The raw data is a Wikipedia dump of English articles that contains a sub-header that contains the word **\"plot\"** (e.g., \"Plot\", \"Plot Summary\", etc.).\n",
    "\n",
    "When the corpus is recreated we have two files:\n",
    "\n",
    "* plots: a text file containing all story plots. Each story plot is given with one sentence per line. Each story is followed by **`<EOS>`** on a line by itself.\n",
    "* titles: a text file containing a list of titles for each article in whih a story plot was found and extracted.\n",
    "\n",
    "The dataset used was uploaded in 23-Mar-2017 14:24 and can be found [here](https://dumps.wikimedia.org/enwiki/). It's a 56Gb `.xml` file zipped as `.bz2` with 14Gb size.\n",
    "\n",
    "When extracted the articles are separated by folders (i.e. \"AA/\", \"AB/\", \"AC/\"), with several stories in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    text_stem = [stemmer.stem(token) for token in text.split(' ')]\n",
    "    text_stem_join = ' '.join(text_stem)\n",
    "    return text_stem_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "***\n",
    "\n",
    "For this example I used the story plots from the \"AB/\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/'\n",
    "plots_filename = 'plots_full.txt'\n",
    "titles_filename = 'titles_full.txt'\n",
    "separator = '<EOS>'\n",
    "\n",
    "with open(dataset_dir + plots_filename, 'r') as file:\n",
    "    corpus = file.readlines()\n",
    "    corpus = corpus[:-1]\n",
    "    corpus = ''.join(corpus)\n",
    "    corpus = corpus.split(separator)\n",
    "\n",
    "with open(dataset_dir + titles_filename, 'r') as file:\n",
    "    titles = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted stories\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of extracted stories: 28149\n"
     ]
    }
   ],
   "source": [
    "N = len(titles)\n",
    "print('Total of extracted stories:', N)\n",
    "# titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot example (A Song of Ice and Fire)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and transforming the stemmed corpus\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_stem = list(map(stem, corpus))\n",
    "tfidf = TfidfVectorizer(norm='l2', use_idf=True, stop_words='english')\n",
    "\n",
    "X = tfidf.fit_transform(corpus_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features extracted: 159562\n"
     ]
    }
   ],
   "source": [
    "print('Total features extracted:', len(tfidf.get_feature_names()))\n",
    "# print(tfidf.get_feature_names())\n",
    "\n",
    "# print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarities of NxN articles\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(i, j):\n",
    "    if (j > i):\n",
    "        return\n",
    "    a[i][j] = cosine_similarity(X[i].toarray(), X[j].toarray())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "a = np.zeros(shape=(N,N))\n",
    "print('Creating sparse matrix...')\n",
    "# for i,j in zip(range(N), range(N)):\n",
    "#     calculate_cosine_similarity(i, j)\n",
    "# for i,j in product(range(N), range(N)):\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            executor.submit(calculate_cosine_similarity, i, j)\n",
    "    \n",
    "print('Done.')\n",
    "print('Creating DataFrame...')\n",
    "df = pd.DataFrame(data=a)\n",
    "print('Done')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 1's to 0's and getting most similar articles\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df == df.max()] = 0\n",
    "\n",
    "most_similar = np.array(df.apply(lambda x: df.columns[x.argmax()], axis = 1))\n",
    "most_similar_values = np.array(df.apply(lambda x: x.max(), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar_values\n",
    "# np.sort(most_similar_values)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most similar articles\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(most_similar)):\n",
    "#     print('Most similar article to', titles[i], 'is:', titles[most_similar[i]], 'with cosine similarity:', df[most_similar[i]].max())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_similar_articles_indexes = np.where(most_similar_values == np.sort(most_similar_values)[-1])[0]\n",
    "top_plot_one_index = most_similar_articles_indexes[0]\n",
    "top_plot_two_index = most_similar[most_similar_articles_indexes[0]]\n",
    "\n",
    "top_title_one = titles[top_plot_one_index]\n",
    "top_title_two = titles[most_similar[most_similar_articles_indexes[0]]]\n",
    "\n",
    "top_corpus_one = corpus[most_similar_articles_indexes[0]]\n",
    "top_corpus_two = corpus[most_similar[most_similar_articles_indexes[0]]]\n",
    "print('Most similar articles:')\n",
    "print(top_title_one, 'and', top_title_two, 'cos:', df.iloc[most_similar_articles_indexes[0]].max())\n",
    "print()\n",
    "print('Plot of', top_title_one)\n",
    "print()\n",
    "print(top_corpus_one)\n",
    "print()\n",
    "print()\n",
    "print('Plot of', top_title_two)\n",
    "print()\n",
    "print(top_corpus_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud of most similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "%matplotlib\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=30, stopwords=tfidf.get_stop_words())\n",
    "wc_ = WordCloud(background_color='white', max_words=30, stopwords=tfidf.get_stop_words())\n",
    "\n",
    "wc.generate(top_corpus_one)\n",
    "wc_.generate(top_corpus_two)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "a = fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(wc)\n",
    "plt.title(top_title_one)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "a = fig.add_subplot(1,2,2)\n",
    "imgplot = plt.imshow(wc_)\n",
    "plt.title(top_title_two)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrix of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
